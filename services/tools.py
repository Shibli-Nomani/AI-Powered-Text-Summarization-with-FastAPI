# services/async_tools.py
import asyncio
from pathlib import Path
import json
from functools import lru_cache
from langchain.tools import tool

# Keep global reference to the summary model
from services.models.summary_model import summarymodel

llm_model = summarymodel

# ----------------------------
# JSON log file path
# ----------------------------
BASE_DIR = Path(__file__).resolve().parent.parent  # go up from utils/ to project root
LOG_FILE = BASE_DIR / "logs" / "file_metadata.json"


# ----------------------------
# Transcript Extractor
# ----------------------------
class TranscriptExtractor:
    """Extracts cleaned transcripts from logs/file_metadata.json."""

    @staticmethod
    def _load_metadata() -> dict:
        """Load metadata from JSON log file."""
        if not LOG_FILE.exists():
            print(f"⚠️ Log file not found: {LOG_FILE}")
            return {}
        with open(LOG_FILE, "r", encoding="utf-8") as f:
            try:
                data = json.load(f)
                if not isinstance(data, dict):
                    print("❌ Invalid JSON structure, expected a dict at root")
                    return {}
                return data
            except json.JSONDecodeError as e:
                print(f"❌ JSON decode error: {e}")
                return {}
            
    @staticmethod
    def _run(file_id: str) -> str:
        """Return the cleaned transcript for a given file_id."""
        if not file_id or not isinstance(file_id, str):
            raise ValueError("file_id must be a non-empty string.")

        logs = TranscriptExtractor._load_metadata()
        default_logs = logs.get("_default", {})

        log_entry = next((v for v in default_logs.values() if v.get("file_name") == file_id), None)
        if not log_entry:
            raise ValueError(f"No metadata found for file_id: {file_id}")

        cleaned_text = log_entry.get("cleaned_transcript", "").strip()
        if not cleaned_text:
            raise ValueError(f"No cleaned transcript available for file_id: {file_id}")

        return cleaned_text


# ----------------------------
# Global instance
# ----------------------------
transcript_extractor = TranscriptExtractor()
@lru_cache(maxsize=128)
def _summarize_task(text: str, max_tokens: int = 50) -> str:
    """
    Generate summary using LLM with caching for repeated calls.

    Args:
        text (str): Transcript text.
        max_tokens (int): Maximum tokens for summary.

    Returns:
        str: Summary generated by the LLM.
    """
    try:
        prompt = f"Summarize the following text (limit {max_tokens} tokens):\n {text}"
        # Using Grok LLM sync method
        response = llm_model.predict(prompt)  # replace .predict() with the actual Grok method
        return response.strip()
    except Exception as e:
        return f"Error summarizing text: {str(e)}"

# ---------------------------- 
# Transcript Summarizer Tool
# ----------------------------

@tool
def transcript_summarizer(file_id: str, max_tokens: int = 50) -> str:
    """
    Extract transcript by file_id and summarize it.

    Args:
        file_id (str): File name in logs.
        max_tokens (int): Maximum tokens for summary.

    Returns:
        str: Summary or error message.
    """
    try:
        # 1️⃣ Extract transcript
        transcript = transcript_extractor._run(file_id)

        # 2️⃣ Generate summary
        summary = _summarize_task(transcript, max_tokens)
        return summary

    except Exception as e:
        return f"Error generating summary: {str(e)}"


